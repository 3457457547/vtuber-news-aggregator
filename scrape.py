#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VTuber News Aggregator - Scraper & HTML Generator
å–å¾—å…ˆ: https://vtuber.atodeyo.com/
å¯¾è±¡ã‚µã‚¤ãƒˆ: ã«ã˜ãƒ›ãƒ­é€Ÿ, Vtuberã¾ã¨ã‚ã‚‹ã‚ˆï½ã‚“, Vtuberã¾ã¨ã‚éƒ¨ï¼, VTuberNews, ã‚„ã‚‰ãŠã‚“ï¼
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
from pathlib import Path
import time
import sys

# ==================== è¨­å®š ====================

BASE_URL = "https://vtuber.atodeyo.com/"
OUTPUT_DIR = Path("public")
CACHE_FILE = Path("cache/scraped_data.json")

# å–å¾—å¯¾è±¡ã‚µã‚¤ãƒˆï¼ˆã‚¯ãƒ©ã‚¹å â†’ ã‚µã‚¤ãƒˆåã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
ALLOWED_SITES = {
    "nhkecr27": "ã«ã˜ãƒ›ãƒ­é€Ÿ",
    "vemogu23": "Vtuberã¾ã¨ã‚ã‚‹ã‚ˆï½ã‚“",
    "vemgco19": "Vtuberã¾ã¨ã‚éƒ¨ï¼",
    "vejwsu12": "VTuberNews",
    "yocgca13": "ã‚„ã‚‰ãŠã‚“ï¼"
}

# é™¤å¤–ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆPRè¨˜äº‹ãªã©ï¼‰
EXCLUDED_CLASSES = ["pr"]

# è¨˜äº‹ä¿æŒæ•°ãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
MAX_ITEMS = 100
ITEMS_PER_PAGE = 50

# ãƒªãƒˆãƒ©ã‚¤è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 5

# ==================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ====================

def ensure_dirs():
    """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    print("âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªå®Œäº†")

def load_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éå»ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    if CACHE_FILE.exists():
        try:
            data = json.loads(CACHE_FILE.read_text(encoding="utf-8"))
            print(f"ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿: {len(data)}ä»¶")
            return data
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç ´æï¼ˆ{e}ï¼‰ã€æ–°è¦ä½œæˆã—ã¾ã™")
            return []
    else:
        print("ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã€æ–°è¦ä½œæˆã—ã¾ã™")
        return []

def save_cache(items):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆåŸå­çš„æ›¸ãè¾¼ã¿ï¼‰"""
    temp_file = CACHE_FILE.with_suffix('.tmp')
    try:
        temp_file.write_text(
            json.dumps(items, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        temp_file.replace(CACHE_FILE)
        print(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {len(items)}ä»¶")
    except Exception as e:
        print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å¤±æ•—: {e}")
        if temp_file.exists():
            temp_file.unlink()

def fetch_html(url):
    """HTMLã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1"
    }
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"ğŸŒ å–å¾—ä¸­ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES}): {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«UTF-8ã«è¨­å®š
            response.encoding = 'utf-8'
            
            print(f"âœ… å–å¾—æˆåŠŸ: {len(response.text)} bytes")
            return response.text
        except requests.exceptions.Timeout:
            print(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        if attempt < MAX_RETRIES - 1:
            wait_time = RETRY_DELAY * (attempt + 1)
            print(f"â³ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
            time.sleep(wait_time)
    
    print("âŒ ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ã¾ã—ãŸ")
    return None

def parse_timeline(html):
    """HTMLã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º"""
    if not html:
        return []
    
    try:
        soup = BeautifulSoup(html, "html.parser")
        timeline = soup.select_one(".timeline")
        
        if not timeline:
            print("âš ï¸ .timelineè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆHTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ï¼‰")
            return []
        
        items = []
        children = timeline.find_all('div', recursive=False)
        print(f"ğŸ” {len(children)}ä»¶ã®è¦ç´ ã‚’æ¤œå‡º")
        
        for idx, child in enumerate(children):
            try:
                classes = child.get('class', [])
                if not classes:
                    continue
                
                site_class = classes[0]
                
                if site_class in EXCLUDED_CLASSES:
                    continue
                
                if site_class not in ALLOWED_SITES:
                    continue
                
                time_p = child.find('p', class_='time')
                article_p = child.find('p', class_='article')
                site_p = child.find('p', class_='site')
                
                if not article_p:
                    continue
                
                article_a = article_p.find('a')
                if not article_a:
                    continue
                
                title = article_a.get_text(strip=True)
                href = article_a.get('href', '')
                time_text = time_p.get_text(strip=True) if time_p else ''
                
                site_name = ALLOWED_SITES.get(site_class, site_class)
                
                if not href or not title:
                    continue
                
                if href.startswith('/'):
                    href = BASE_URL.rstrip('/') + href
                elif not href.startswith('http'):
                    continue
                
                item = {
                    "title": title[:200],
                    "url": href,
                    "site": site_name,
                    "site_class": site_class,
                    "time_text": time_text[:50],
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                    "id": abs(hash(href))
                }
                
                items.append(item)
                
            except Exception as e:
                print(f"âš ï¸ è¦ç´ {idx}ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"âœ… {len(items)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆå¯¾è±¡ã‚µã‚¤ãƒˆã®ã¿ï¼‰")
        
        site_counts = {}
        for item in items:
            site = item['site']
            site_counts[site] = site_counts.get(site, 0) + 1
        
        for site, count in site_counts.items():
            print(f"   - {site}: {count}ä»¶")
        
        return items
        
    except Exception as e:
        print(f"âŒ HTMLè§£æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def dedupe_and_merge(old_items, new_items):
    """æ–°æ—§è¨˜äº‹ã‚’ãƒãƒ¼ã‚¸ã—ã€é‡è¤‡ã‚’å‰Šé™¤"""
    existing_ids = {item.get("id") for item in old_items if "id" in item}
    
    unique_new = [
        item for item in new_items 
        if item.get("id") not in existing_ids
    ]
    
    print(f"ğŸ†• æ–°è¦è¨˜äº‹: {len(unique_new)}ä»¶")
    
    merged = unique_new + old_items
    trimmed = merged[:MAX_ITEMS]
    
    if len(merged) > MAX_ITEMS:
        print(f"âœ‚ï¸ {len(merged) - MAX_ITEMS}ä»¶ã‚’å‰Šé™¤ï¼ˆä¸Šé™{MAX_ITEMS}ä»¶ï¼‰")
    
    return trimmed

# ==================== HTMLç”Ÿæˆ ====================

def render_header(page_title="VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚ | æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§"):
    """HTMLãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆå…±é€šï¼‰"""
    return f"""<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="description" content="VTuberã¾ã¨ã‚ã‚µã‚¤ãƒˆã®æœ€æ–°æƒ…å ±ã‚’ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯ã€‚ã«ã˜ãƒ›ãƒ­é€Ÿã€ã‚„ã‚‰ãŠã‚“ç­‰ã®äººæ°—ã‚µã‚¤ãƒˆã‹ã‚‰2æ™‚é–“ã”ã¨ã«è‡ªå‹•åé›†ã€‚">
<meta name="keywords" content="VTuber,ã¾ã¨ã‚,ã«ã˜ã•ã‚“ã˜,ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–,æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹,ã‚„ã‚‰ãŠã‚“,ã«ã˜ãƒ›ãƒ­é€Ÿ">
<title>{page_title}</title>
<link rel="stylesheet" href="/style.css">
<link rel="canonical" href="https://3457457547.github.io/vtuber-news-aggregator/">
<meta property="og:title" content="VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚">
<meta property="og:description" content="VTuberé–¢é€£ã®æœ€æ–°ã¾ã¨ã‚è¨˜äº‹ã‚’ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯">
<meta property="og:type" content="website">
<meta property="og:url" content="https://3457457547.github.io/vtuber-news-aggregator/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚">
</head>
<body>
<header>
<div class="container">
<h1>ğŸ“° VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚</h1>
<p class="subtitle">äººæ°—VTuberã¾ã¨ã‚ã‚µã‚¤ãƒˆã®æœ€æ–°æƒ…å ±ã‚’2æ™‚é–“ã”ã¨ã«æ›´æ–°</p>
</div>
</header>
<main class="container">"""

def render_footer():
    """HTMLãƒ•ãƒƒã‚¿ãƒ¼ï¼ˆå…±é€šï¼‰"""
    now = datetime.now(timezone.utc)
    return f"""
</main>
<footer class="site-footer">
<div class="container">
<p>&copy; 2024 VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚ | æœ€çµ‚æ›´æ–°: {now.strftime('%Y-%m-%d %H:%M')} UTC</p>
<p class="sources">æƒ…å ±å…ƒ: {", ".join(ALLOWED_SITES.values())}</p>
</div>
</footer>
</body>
</html>"""

def render_article(item):
    """è¨˜äº‹ã‚«ãƒ¼ãƒ‰HTML"""
    return f"""<article class="post">
<time>{item.get('time_text', '')}</time>
<h2><a href="{item['url']}" target="_blank" rel="noopener noreferrer">{item['title']}</a></h2>
<p class="source">{item['site']}</p>
</article>
"""

def render_ad_block():
    """åºƒå‘Šãƒ–ãƒ­ãƒƒã‚¯"""
    return """<div class="ad-block">
<!-- nendåºƒå‘Šã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«æŒ¿å…¥ -->
<p style="color:#999;font-size:0.9rem;">åºƒå‘Šã‚¨ãƒªã‚¢ï¼ˆnendå¯©æŸ»é€šéå¾Œã«è¡¨ç¤ºï¼‰</p>
</div>
"""

def render_index(items):
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸HTMLç”Ÿæˆ"""
    html = render_header()
    html += '<div class="list">\n'
    
    for i, item in enumerate(items[:ITEMS_PER_PAGE]):
        html += render_article(item)
        if (i + 1) % 10 == 0 and i < ITEMS_PER_PAGE - 1:
            html += render_ad_block()
    
    html += '</div>\n'
    
    if len(items) > ITEMS_PER_PAGE:
        html += '<nav class="pager"><a href="/page2.html">éå»ã®è¨˜äº‹ â†’</a></nav>\n'
    
    html += render_footer()
    return html

def render_page2(items):
    """2ãƒšãƒ¼ã‚¸ç›®HTMLç”Ÿæˆ"""
    html = render_header(page_title="éå»ã®è¨˜äº‹ - VTuberã¾ã¨ã‚ã®ã¾ã¨ã‚")
    html += '<div class="list">\n'
    
    page2_items = items[ITEMS_PER_PAGE:ITEMS_PER_PAGE * 2]
    
    for i, item in enumerate(page2_items):
        html += render_article(item)
        if (i + 1) % 10 == 0 and i < len(page2_items) - 1:
            html += render_ad_block()
    
    html += '</div>\n'
    html += '<nav class="pager"><a href="/index.html">â† æœ€æ–°è¨˜äº‹ã¸</a></nav>\n'
    html += render_footer()
    return html

def generate_css():
    """ã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    return """:root {
  --primary: #0066cc;
  --text: #1a1a1a;
  --text-light: #666;
  --border: #e5e5e5;
  --bg: #fafafa;
}

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  color: var(--text);
  background: var(--bg);
}

header {
  background: #fff;
  border-bottom: 1px solid var(--border);
  padding: 1.5rem 0;
  box-shadow: 0 1px 3px rgba(0,0,0,0.05);
  position: sticky;
  top: 0;
  z-index: 100;
}

.container {
  max-width: 900px;
  margin: 0 auto;
  padding: 0 1.5rem;
}

h1 {
  font-size: 1.75rem;
  font-weight: 700;
  color: var(--primary);
  margin-bottom: 0.25rem;
}

.subtitle {
  color: var(--text-light);
  font-size: 0.95rem;
}

main {
  padding: 2rem 0;
  min-height: 60vh;
}

.list {
  display: flex;
  flex-direction: column;
  gap: 1rem;
}

article.post {
  background: #fff;
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 1.25rem;
  transition: all 0.2s ease;
}

article.post:hover {
  box-shadow: 0 4px 12px rgba(0,0,0,0.08);
  transform: translateY(-2px);
}

time {
  font-size: 0.85rem;
  color: var(--text-light);
  display: block;
  margin-bottom: 0.5rem;
}

article.post h2 {
  font-size: 1.1rem;
  font-weight: 500;
  line-height: 1.5;
  margin-bottom: 0.5rem;
}

article.post h2 a {
  color: var(--text);
  text-decoration: none;
}

article.post h2 a:hover {
  color: var(--primary);
  text-decoration: underline;
}

.source {
  font-size: 0.85rem;
  color: var(--text-light);
  background: var(--bg);
  display: inline-block;
  padding: 0.25rem 0.75rem;
  border-radius: 4px;
  font-weight: 500;
}

.ad-block {
  margin: 1.5rem 0;
  padding: 1.5rem;
  background: #f9f9f9;
  border: 1px dashed var(--border);
  border-radius: 8px;
  text-align: center;
  min-height: 120px;
  display: flex;
  align-items: center;
  justify-content: center;
}

nav.pager {
  text-align: center;
  margin: 3rem 0;
}

nav.pager a {
  display: inline-block;
  padding: 0.75rem 2rem;
  background: var(--primary);
  color: #fff;
  text-decoration: none;
  border-radius: 6px;
  font-weight: 500;
  transition: all 0.2s ease;
}

nav.pager a:hover {
  background: #0052a3;
  transform: translateY(-2px);
  box-shadow: 0 4px 8px rgba(0,102,204,0.3);
}

.site-footer {
  background: #fff;
  border-top: 1px solid var(--border);
  padding: 2rem 0;
  margin-top: 4rem;
  text-align: center;
  color: var(--text-light);
  font-size: 0.9rem;
}

.site-footer .sources {
  margin-top: 0.5rem;
  font-size: 0.85rem;
}

@media (max-width: 768px) {
  .container {
    padding: 0 1rem;
  }
  h1 {
    font-size: 1.4rem;
  }
  .subtitle {
    font-size: 0.85rem;
  }
  article.post {
    padding: 1rem;
  }
  article.post h2 {
    font-size: 1rem;
  }
  nav.pager a {
    padding: 0.6rem 1.5rem;
    font-size: 0.95rem;
  }
  .ad-block {
    min-height: 100px;
    padding: 1rem;
  }
}
"""

def generate_robots_txt():
    """robots.txtç”Ÿæˆ"""
    return """User-agent: *
Allow: /

Sitemap: https://3457457547.github.io/vtuber-news-aggregator/sitemap.xml
"""

def generate_sitemap(items):
    """sitemap.xmlç”Ÿæˆ"""
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    sitemap += '  <url>\n'
    sitemap += '    <loc>https://3457457547.github.io/vtuber-news-aggregator/</loc>\n'
    sitemap += '    <changefreq>hourly</changefreq>\n'
    sitemap += '    <priority>1.0</priority>\n'
    sitemap += '  </url>\n'
    
    if len(items) > ITEMS_PER_PAGE:
        sitemap += '  <url>\n'
        sitemap += '    <loc>https://3457457547.github.io/vtuber-news-aggregator/page2.html</loc>\n'
        sitemap += '    <changefreq>daily</changefreq>\n'
        sitemap += '    <priority>0.8</priority>\n'
        sitemap += '  </url>\n'
    
    sitemap += '</urlset>'
    return sitemap

def write_files(items):
    """ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“„ index.html ç”Ÿæˆä¸­...")
    (OUTPUT_DIR / "index.html").write_text(render_index(items), encoding="utf-8")
    
    print("ğŸ“„ page2.html ç”Ÿæˆä¸­...")
    (OUTPUT_DIR / "page2.html").write_text(render_page2(items), encoding="utf-8")
    
    print("ğŸ¨ style.css ç”Ÿæˆä¸­...")
    (OUTPUT_DIR / "style.css").write_text(generate_css(), encoding="utf-8")
    
    print("ğŸ” robots.txt ç”Ÿæˆä¸­...")
    (OUTPUT_DIR / "robots.txt").write_text(generate_robots_txt(), encoding="utf-8")
    
    print("ğŸ—ºï¸ sitemap.xml ç”Ÿæˆä¸­...")
    (OUTPUT_DIR / "sitemap.xml").write_text(generate_sitemap(items), encoding="utf-8")
    
    print("âœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("VTuber News Aggregator - èµ·å‹•")
    print("=" * 60)
    
    ensure_dirs()
    old_items = load_cache()
    html = fetch_html(BASE_URL)
    
    if not html:
        print("âš ï¸ HTMLå–å¾—å¤±æ•— - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç”Ÿæˆã—ã¾ã™")
        if old_items:
            write_files(old_items)
            print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç”Ÿæˆå®Œäº†")
        else:
            print("âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚å­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)
        return
    
    new_items = parse_timeline(html)
    
    if not new_items:
        print("âš ï¸ è¨˜äº‹ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        if old_items:
            print("ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç”Ÿæˆã—ã¾ã™")
            write_files(old_items)
            print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç”Ÿæˆå®Œäº†")
        else:
            print("âŒ ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            sys.exit(1)
        return
    
    merged_items = dedupe_and_merge(old_items, new_items)
    save_cache(merged_items)
    write_files(merged_items)
    
    print("=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print(f"ğŸ“Š ç·è¨˜äº‹æ•°: {len(merged_items)}ä»¶")
    print("=" * 60)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
