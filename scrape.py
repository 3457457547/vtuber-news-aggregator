#!/usr/bin/env python3
"""
æ–°äººVTuberç™ºæ˜ã‚µã‚¤ãƒˆ - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
YouTube Data API v3 ã‚’ä½¿ã£ã¦æ–°äººVTuberã‚’è‡ªå‹•ç™ºæ˜ã—ã€
é™çš„HTMLã‚µã‚¤ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

ãƒ•ãƒ­ãƒ¼:
  1. YouTube API ã§æ–°äººVTuberå€™è£œã‚’æ¤œç´¢
  2. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç™»éŒ²è€…æ•°ãƒ»é–‹è¨­æ—¥ãƒ»ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åº¦ï¼‰
  3. å€™è£œãƒªã‚¹ãƒˆã‚’JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
  4. æ‰¿èªæ¸ˆã¿VTuberã®ç´¹ä»‹ãƒšãƒ¼ã‚¸ã‚’HTMLç”Ÿæˆ
  5. Git push â†’ GitHub Pages ã§è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
"""

import os
import sys
import json
import re
import math
import hashlib
from datetime import datetime, timedelta, timezone
from pathlib import Path
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
from urllib.parse import urlencode, quote

# ============================================================
# è¨­å®š
# ============================================================

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SEARCH_QUERIES = [
    "æ–°äººVTuber",
    "VTuberãƒ‡ãƒ“ãƒ¥ãƒ¼",
    "åˆé…ä¿¡ VTuber",
    "å€‹äººå‹¢VTuber ãƒ‡ãƒ“ãƒ¥ãƒ¼",
    "æ–°äººVtuber è‡ªå·±ç´¹ä»‹",
]

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
MAX_SUBSCRIBERS = 1000        # ç™»éŒ²è€…æ•°ä¸Šé™
MAX_CHANNEL_AGE_DAYS = 90     # ãƒãƒ£ãƒ³ãƒãƒ«é–‹è¨­ã‹ã‚‰ã®æ—¥æ•°ä¸Šé™
MIN_VIDEOS = 3                # æœ€ä½å‹•ç”»æ•°
MAX_DAYS_SINCE_LAST_VIDEO = 30  # æœ€çµ‚æŠ•ç¨¿ã‹ã‚‰ã®æ—¥æ•°ä¸Šé™

# VTuberåˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«å or èª¬æ˜æ–‡ã«å«ã¾ã‚Œã‚‹ã‹ï¼‰
VTUBER_KEYWORDS = [
    "vtuber", "ãƒ–ã‚¤ãƒãƒ¥ãƒ¼ãƒãƒ¼", "Vãƒãƒ¥ãƒ¼ãƒãƒ¼",
    "ãƒãƒ¼ãƒãƒ£ãƒ«", "virtual", "ãƒãƒ", "ãƒ‘ãƒ‘",
    "Live2D", "live2d", "é…ä¿¡è€…", "ã‚²ãƒ¼ãƒ å®Ÿæ³",
    "æ­Œã£ã¦ã¿ãŸ", "åˆé…ä¿¡", "ãƒ‡ãƒ“ãƒ¥ãƒ¼",
]

# å‡ºåŠ›è¨­å®š
SITE_NAME = "æ–°äººVTuberç™ºæ˜æ‰€"
SITE_TAGLINE = "ã‚ãªãŸã®æ¨ã—ã«ãªã‚‹æ–°äººã€ã“ã“ã§è¦‹ã¤ã‹ã‚‹"
SITE_URL = "https://vtuber-matome.net"
ITEMS_PER_PAGE = 20

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
CACHE_DIR = Path("cache")
PUBLIC_DIR = Path("docs")
CANDIDATES_FILE = CACHE_DIR / "candidates.json"
APPROVED_FILE = CACHE_DIR / "approved.json"

# ChatGPT APIï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æœªè¨­å®šãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼‰
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = "gpt-4o-mini"

# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def ensure_dirs():
    """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    CACHE_DIR.mkdir(exist_ok=True)
    PUBLIC_DIR.mkdir(exist_ok=True)


def load_json(path: Path, default=None):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if path.exists():
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return default if default is not None else []


def save_json(path: Path, data):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def api_request(endpoint: str, params: dict) -> dict:
    """YouTube Data API ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ã‚‹"""
    params["key"] = YOUTUBE_API_KEY
    url = f"{YOUTUBE_API_BASE}/{endpoint}?{urlencode(params)}"
    req = Request(url, headers={"Accept": "application/json"})
    try:
        with urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except HTTPError as e:
        print(f"[ERROR] YouTube API {endpoint}: {e.code} {e.reason}")
        if e.code == 403:
            print("[ERROR] APIã‚¯ã‚©ãƒ¼ã‚¿è¶…éã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        return {}
    except URLError as e:
        print(f"[ERROR] Network error: {e}")
        return {}


def parse_iso8601(date_str: str) -> datetime:
    """ISO8601æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹"""
    # Python 3.11+ ã® fromisoformat ã§å¯¾å¿œ
    date_str = date_str.replace("Z", "+00:00")
    return datetime.fromisoformat(date_str)


def days_ago(date_str: str) -> int:
    """æŒ‡å®šæ—¥ä»˜ã‹ã‚‰ä»Šæ—¥ã¾ã§ã®æ—¥æ•°"""
    dt = parse_iso8601(date_str)
    now = datetime.now(timezone.utc)
    return (now - dt).days


def format_subscriber_count(count: int) -> str:
    """ç™»éŒ²è€…æ•°ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«"""
    if count >= 10000:
        return f"{count / 10000:.1f}ä¸‡äºº"
    elif count >= 1000:
        return f"{count / 1000:.1f}åƒäºº"
    return f"{count}äºº"


def format_date_jp(date_str: str) -> str:
    """æ—¥ä»˜ã‚’æ—¥æœ¬èªå½¢å¼ã«"""
    dt = parse_iso8601(date_str)
    return dt.strftime("%Yå¹´%mæœˆ%dæ—¥")


def channel_id_hash(channel_id: str) -> str:
    """ãƒãƒ£ãƒ³ãƒãƒ«IDã‹ã‚‰çŸ­ã„ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ï¼‰"""
    return hashlib.md5(channel_id.encode()).hexdigest()[:8]


# ============================================================
# YouTube API: æ–°äººVTuberæ¤œç´¢
# ============================================================

def search_channels(query: str, max_results: int = 20) -> list:
    """
    YouTubeæ¤œç´¢APIã§ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¤œç´¢
    ã‚¯ã‚©ãƒ¼ã‚¿ã‚³ã‚¹ãƒˆ: 100/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    """
    published_after = (datetime.now(timezone.utc) - timedelta(days=MAX_CHANNEL_AGE_DAYS)).strftime("%Y-%m-%dT%H:%M:%SZ")

    data = api_request("search", {
        "part": "snippet",
        "q": query,
        "type": "channel",
        "maxResults": max_results,
        "publishedAfter": published_after,
        "order": "date",
        "regionCode": "JP",
        "relevanceLanguage": "ja",
    })

    channels = []
    for item in data.get("items", []):
        channels.append({
            "channel_id": item["snippet"]["channelId"],
            "title": item["snippet"]["title"],
            "description": item["snippet"]["description"],
            "thumbnail": item["snippet"]["thumbnails"].get("medium", {}).get("url", ""),
            "published_at": item["snippet"]["publishedAt"],
        })

    return channels


def get_channel_details(channel_ids: list) -> dict:
    """
    ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°æƒ…å ±ï¼ˆç™»éŒ²è€…æ•°ã€å‹•ç”»æ•°ãªã©ï¼‰ã‚’å–å¾—
    ã‚¯ã‚©ãƒ¼ã‚¿ã‚³ã‚¹ãƒˆ: 1/ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆæœ€å¤§50ãƒãƒ£ãƒ³ãƒãƒ«/ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    """
    if not channel_ids:
        return {}

    # 50ä»¶ãšã¤åˆ†å‰²
    results = {}
    for i in range(0, len(channel_ids), 50):
        batch = channel_ids[i:i+50]
        data = api_request("channels", {
            "part": "snippet,statistics,brandingSettings",
            "id": ",".join(batch),
        })

        for item in data.get("items", []):
            cid = item["id"]
            stats = item.get("statistics", {})
            snippet = item.get("snippet", {})
            branding = item.get("brandingSettings", {}).get("channel", {})

            # ç™»éŒ²è€…æ•°ãŒéå…¬é–‹ã®å ´åˆ
            sub_count = int(stats.get("subscriberCount", 0))
            if stats.get("hiddenSubscriberCount", False):
                sub_count = -1  # éå…¬é–‹

            results[cid] = {
                "channel_id": cid,
                "title": snippet.get("title", ""),
                "description": snippet.get("description", ""),
                "custom_url": snippet.get("customUrl", ""),
                "thumbnail": snippet.get("thumbnails", {}).get("medium", {}).get("url", ""),
                "published_at": snippet.get("publishedAt", ""),
                "subscriber_count": sub_count,
                "video_count": int(stats.get("videoCount", 0)),
                "view_count": int(stats.get("viewCount", 0)),
                "keywords": branding.get("keywords", ""),
            }

    return results


def get_latest_videos(channel_id: str, max_results: int = 5) -> list:
    """
    ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»ã‚’å–å¾—
    ã‚¯ã‚©ãƒ¼ã‚¿ã‚³ã‚¹ãƒˆ: 100/ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆsearch APIã‚’ä½¿ç”¨ï¼‰
    â€» ã‚¯ã‚©ãƒ¼ã‚¿ç¯€ç´„ã®ãŸã‚ã€å€™è£œç¢ºå®šå¾Œã®ã¿å‘¼ã¶
    """
    data = api_request("search", {
        "part": "snippet",
        "channelId": channel_id,
        "type": "video",
        "maxResults": max_results,
        "order": "date",
    })

    videos = []
    for item in data.get("items", []):
        videos.append({
            "video_id": item["id"]["videoId"],
            "title": item["snippet"]["title"],
            "thumbnail": item["snippet"]["thumbnails"].get("medium", {}).get("url", ""),
            "published_at": item["snippet"]["publishedAt"],
        })

    return videos


def get_video_details(video_ids: list) -> dict:
    """
    å‹•ç”»ã®è©³ç´°æƒ…å ±ï¼ˆå†ç”Ÿå›æ•°ãªã©ï¼‰ã‚’å–å¾—
    ã‚¯ã‚©ãƒ¼ã‚¿ã‚³ã‚¹ãƒˆ: 1/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    """
    if not video_ids:
        return {}

    results = {}
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = api_request("videos", {
            "part": "snippet,statistics,contentDetails",
            "id": ",".join(batch),
        })

        for item in data.get("items", []):
            vid = item["id"]
            stats = item.get("statistics", {})
            results[vid] = {
                "video_id": vid,
                "title": item["snippet"]["title"],
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
                "duration": item.get("contentDetails", {}).get("duration", ""),
            }

    return results


# ============================================================
# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
# ============================================================

def is_likely_vtuber(channel: dict) -> bool:
    """VTuberã®å¯èƒ½æ€§ãŒé«˜ã„ã‹åˆ¤å®š"""
    text = (channel.get("title", "") + " " +
            channel.get("description", "") + " " +
            channel.get("keywords", "")).lower()

    return any(kw.lower() in text for kw in VTUBER_KEYWORDS)


def passes_filters(channel: dict) -> tuple:
    """
    ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
    Returns: (passes: bool, reason: str)
    """
    # ç™»éŒ²è€…æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆéå…¬é–‹ã¯é€šã™ï¼‰
    sub_count = channel.get("subscriber_count", 0)
    if sub_count > MAX_SUBSCRIBERS and sub_count != -1:
        return False, f"ç™»éŒ²è€…æ•°ãŒ{MAX_SUBSCRIBERS}äººã‚’è¶…ãˆã¦ã„ã‚‹ï¼ˆ{sub_count}äººï¼‰"

    # ãƒãƒ£ãƒ³ãƒãƒ«å¹´é½¢ãƒã‚§ãƒƒã‚¯
    pub_date = channel.get("published_at", "")
    if pub_date:
        age = days_ago(pub_date)
        if age > MAX_CHANNEL_AGE_DAYS:
            return False, f"ãƒãƒ£ãƒ³ãƒãƒ«é–‹è¨­ã‹ã‚‰{age}æ—¥çµŒéï¼ˆä¸Šé™{MAX_CHANNEL_AGE_DAYS}æ—¥ï¼‰"

    # å‹•ç”»æ•°ãƒã‚§ãƒƒã‚¯
    video_count = channel.get("video_count", 0)
    if video_count < MIN_VIDEOS:
        return False, f"å‹•ç”»æ•°ãŒ{video_count}æœ¬ï¼ˆæœ€ä½{MIN_VIDEOS}æœ¬å¿…è¦ï¼‰"

    # VTuberåˆ¤å®š
    if not is_likely_vtuber(channel):
        return False, "VTuberé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„"

    return True, "OK"


# ============================================================
# ChatGPT API: ç´¹ä»‹æ–‡ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ============================================================

def generate_introduction(channel: dict, videos: list) -> str:
    """ChatGPT APIã§ç´¹ä»‹æ–‡ã‚’è‡ªå‹•ç”Ÿæˆ"""
    if not OPENAI_API_KEY:
        return generate_fallback_introduction(channel)

    video_titles = "\n".join([f"- {v['title']}" for v in videos[:5]])

    prompt = f"""ä»¥ä¸‹ã®VTuberãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã‚’ã‚‚ã¨ã«ã€å¿œæ´ã™ã‚‹æ°—æŒã¡ã‚’è¾¼ã‚ãŸç´¹ä»‹æ–‡ã‚’3è¡Œã§æ›¸ã„ã¦ãã ã•ã„ã€‚
ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§æ˜ã‚‹ã„ãƒˆãƒ¼ãƒ³ã§ã€è¦–è´è€…ãŒã€Œè¦‹ã¦ã¿ãŸã„ã€ã¨æ€ã†ã‚ˆã†ãªç´¹ä»‹ã«ã—ã¦ãã ã•ã„ã€‚

ãƒãƒ£ãƒ³ãƒãƒ«å: {channel['title']}
ãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜: {channel.get('description', 'ãªã—')[:200]}
æœ€è¿‘ã®å‹•ç”»:
{video_titles}
ç™»éŒ²è€…æ•°: {format_subscriber_count(channel.get('subscriber_count', 0))}

ãƒ«ãƒ¼ãƒ«:
- 3è¡Œä»¥å†…
- çµµæ–‡å­—ã¯1ã€œ2å€‹ã¾ã§
- ã€Œå¿œæ´ã—ã¦ã„ã¾ã™ã€çš„ãªå‰å‘ããªç· ã‚
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¯ä½¿ã‚ãªã„"""

    try:
        req_body = json.dumps({
            "model": OPENAI_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 200,
            "temperature": 0.7,
        }).encode("utf-8")

        req = Request(
            "https://api.openai.com/v1/chat/completions",
            data=req_body,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {OPENAI_API_KEY}",
            },
        )
        with urlopen(req, timeout=30) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            return data["choices"][0]["message"]["content"].strip()

    except Exception as e:
        print(f"[WARN] ChatGPT API error: {e}")
        return generate_fallback_introduction(channel)


def generate_fallback_introduction(channel: dict) -> str:
    """ChatGPT APIãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç´¹ä»‹æ–‡"""
    title = channel.get("title", "åå‰ä¸æ˜")
    desc = channel.get("description", "")[:100]
    if desc:
        return f"{title}ã•ã‚“ãŒVTuberã¨ã—ã¦ãƒ‡ãƒ“ãƒ¥ãƒ¼ï¼ {desc.split(chr(10))[0]}"
    return f"{title}ã•ã‚“ãŒVTuberã¨ã—ã¦ãƒ‡ãƒ“ãƒ¥ãƒ¼ï¼ ãœã²ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"


# ============================================================
# ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯: å€™è£œåé›†
# ============================================================

def collect_candidates() -> list:
    """
    å…¨æ¤œç´¢ã‚¯ã‚¨ãƒªã§æ–°äººVTuberå€™è£œã‚’åé›†ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    """
    print("=" * 60)
    print("æ–°äººVTuberå€™è£œã‚’åé›†ä¸­...")
    print("=" * 60)

    # æ—¢å­˜ã®å€™è£œãƒ»æ‰¿èªæ¸ˆã¿ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
    existing_candidates = load_json(CANDIDATES_FILE, [])
    approved = load_json(APPROVED_FILE, [])

    existing_ids = {c["channel_id"] for c in existing_candidates}
    approved_ids = {a["channel_id"] for a in approved}

    all_channel_ids = []
    channel_snippets = {}  # channel_id -> search snippet

    # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢
    for query in SEARCH_QUERIES:
        print(f"\næ¤œç´¢ä¸­: ã€Œ{query}ã€")
        results = search_channels(query, max_results=10)
        print(f"  â†’ {len(results)}ä»¶ãƒ’ãƒƒãƒˆ")

        for ch in results:
            cid = ch["channel_id"]
            if cid not in existing_ids and cid not in approved_ids:
                if cid not in channel_snippets:
                    all_channel_ids.append(cid)
                    channel_snippets[cid] = ch

    if not all_channel_ids:
        print("\næ–°ã—ã„å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return existing_candidates

    # é‡è¤‡ã‚’é™¤å»ã—ãŸæ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ã‚’å–å¾—
    unique_ids = list(set(all_channel_ids))
    print(f"\næ–°è¦ãƒãƒ£ãƒ³ãƒãƒ« {len(unique_ids)}ä»¶ã®è©³ç´°ã‚’å–å¾—ä¸­...")
    details = get_channel_details(unique_ids)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    new_candidates = []
    for cid, detail in details.items():
        # ã‚¹ãƒ‹ãƒšãƒƒãƒˆæƒ…å ±ã‚’ãƒãƒ¼ã‚¸
        snippet = channel_snippets.get(cid, {})
        detail["thumbnail"] = detail.get("thumbnail") or snippet.get("thumbnail", "")

        passes, reason = passes_filters(detail)
        if passes:
            detail["discovered_at"] = datetime.now(timezone.utc).isoformat()
            detail["status"] = "pending"  # pending / approved / rejected
            new_candidates.append(detail)
            print(f"  âœ… {detail['title']}ï¼ˆ{format_subscriber_count(detail.get('subscriber_count', 0))}ï¼‰")
        else:
            print(f"  âŒ {detail.get('title', cid)}: {reason}")

    # æ—¢å­˜å€™è£œã¨ãƒãƒ¼ã‚¸
    merged = existing_candidates + new_candidates
    print(f"\nå€™è£œåˆè¨ˆ: {len(merged)}ä»¶ï¼ˆæ–°è¦ {len(new_candidates)}ä»¶ï¼‰")

    return merged


# ============================================================
# æ‰¿èªå‡¦ç†
# ============================================================

def approve_candidate(candidates: list, channel_id: str) -> tuple:
    """
    å€™è£œã‚’æ‰¿èªã—ã¦æ‰¿èªãƒªã‚¹ãƒˆã«ç§»å‹•
    Returns: (updated_candidates, approved_entry)
    """
    approved = load_json(APPROVED_FILE, [])

    target = None
    remaining = []
    for c in candidates:
        if c["channel_id"] == channel_id:
            target = c
        else:
            remaining.append(c)

    if not target:
        print(f"[WARN] ãƒãƒ£ãƒ³ãƒãƒ« {channel_id} ãŒå€™è£œãƒªã‚¹ãƒˆã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return candidates, None

    # æœ€æ–°å‹•ç”»ã‚’å–å¾—
    print(f"ã€Œ{target['title']}ã€ã®æœ€æ–°å‹•ç”»ã‚’å–å¾—ä¸­...")
    videos = get_latest_videos(channel_id, max_results=5)
    target["latest_videos"] = videos

    # ç´¹ä»‹æ–‡ã‚’ç”Ÿæˆ
    print(f"ç´¹ä»‹æ–‡ã‚’ç”Ÿæˆä¸­...")
    target["introduction"] = generate_introduction(target, videos)

    # æ‰¿èª
    target["status"] = "approved"
    target["approved_at"] = datetime.now(timezone.utc).isoformat()
    approved.append(target)

    save_json(APPROVED_FILE, approved)
    print(f"âœ… {target['title']} ã‚’æ‰¿èªã—ã¾ã—ãŸ")

    return remaining, target


def auto_approve_from_spreadsheet():
    """
    Google SpreadsheetçµŒç”±ã®æ‰¿èªã‚’ãƒã‚§ãƒƒã‚¯
    ï¼ˆApps ScriptãŒ approved.json ã«ç›´æ¥æ›¸ãè¾¼ã‚€æƒ³å®šï¼‰
    â€» å°†æ¥å®Ÿè£…ã€‚ç¾åœ¨ã¯CLIã‹ã‚‰ approve ã‚³ãƒãƒ³ãƒ‰ã§ä»£ç”¨
    """
    pass


# ============================================================
# HTMLç”Ÿæˆ
# ============================================================

def render_css() -> str:
    """CSSã‚’ç”Ÿæˆ"""
    return """
:root {
  --primary: #6C5CE7;
  --primary-light: #A29BFE;
  --accent: #FD79A8;
  --accent-light: #FDCB6E;
  --bg: #F8F9FA;
  --card-bg: #FFFFFF;
  --text: #2D3436;
  --text-light: #636E72;
  --border: #E9ECEF;
  --shadow: 0 2px 12px rgba(0,0,0,0.08);
  --shadow-hover: 0 8px 25px rgba(108,92,231,0.15);
  --radius: 12px;
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
  font-family: "Hiragino Kaku Gothic ProN", "Noto Sans JP", "Segoe UI", sans-serif;
  background: var(--bg);
  color: var(--text);
  line-height: 1.7;
  min-height: 100vh;
}

/* ãƒ˜ãƒƒãƒ€ãƒ¼ */
.header {
  background: linear-gradient(135deg, var(--primary), #4834D4);
  color: white;
  padding: 2rem 1rem;
  text-align: center;
  position: relative;
  overflow: hidden;
}
.header::before {
  content: '';
  position: absolute;
  top: -50%;
  left: -50%;
  width: 200%;
  height: 200%;
  background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
  animation: pulse 4s ease-in-out infinite;
}
@keyframes pulse {
  0%, 100% { transform: scale(1); opacity: 0.5; }
  50% { transform: scale(1.1); opacity: 1; }
}
.header h1 {
  font-size: 1.8rem;
  position: relative;
  z-index: 1;
  text-shadow: 0 2px 4px rgba(0,0,0,0.2);
}
.header p {
  font-size: 0.95rem;
  opacity: 0.9;
  margin-top: 0.5rem;
  position: relative;
  z-index: 1;
}

/* ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ */
.container {
  max-width: 800px;
  margin: 0 auto;
  padding: 1.5rem 1rem;
}

/* ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ« */
.section-title {
  font-size: 1.2rem;
  color: var(--primary);
  margin: 2rem 0 1rem;
  padding-bottom: 0.5rem;
  border-bottom: 2px solid var(--primary-light);
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

/* VTuberã‚«ãƒ¼ãƒ‰ */
.vtuber-card {
  background: var(--card-bg);
  border-radius: var(--radius);
  box-shadow: var(--shadow);
  padding: 1.2rem;
  margin-bottom: 1rem;
  transition: all 0.3s ease;
  border: 1px solid var(--border);
  animation: fadeInUp 0.5s ease both;
}
.vtuber-card:hover {
  box-shadow: var(--shadow-hover);
  transform: translateY(-2px);
  border-color: var(--primary-light);
}

@keyframes fadeInUp {
  from { opacity: 0; transform: translateY(20px); }
  to { opacity: 1; transform: translateY(0); }
}

.card-header {
  display: flex;
  gap: 1rem;
  align-items: flex-start;
}
.card-thumbnail {
  width: 64px;
  height: 64px;
  border-radius: 50%;
  object-fit: cover;
  border: 3px solid var(--primary-light);
  flex-shrink: 0;
}
.card-info {
  flex: 1;
  min-width: 0;
}
.card-name {
  font-size: 1.1rem;
  font-weight: bold;
  color: var(--text);
  margin-bottom: 0.25rem;
}
.card-name a {
  color: inherit;
  text-decoration: none;
}
.card-name a:hover {
  color: var(--primary);
}
.card-meta {
  display: flex;
  flex-wrap: wrap;
  gap: 0.5rem;
  font-size: 0.8rem;
  color: var(--text-light);
  margin-bottom: 0.5rem;
}
.card-meta span {
  display: inline-flex;
  align-items: center;
  gap: 0.2rem;
}
.card-intro {
  font-size: 0.9rem;
  color: var(--text);
  line-height: 1.8;
  margin-top: 0.75rem;
  padding-top: 0.75rem;
  border-top: 1px solid var(--border);
}

/* å‹•ç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ */
.card-videos {
  margin-top: 0.75rem;
  padding-top: 0.75rem;
  border-top: 1px solid var(--border);
}
.card-videos-title {
  font-size: 0.8rem;
  color: var(--text-light);
  margin-bottom: 0.5rem;
}
.video-link {
  display: block;
  font-size: 0.85rem;
  color: var(--primary);
  text-decoration: none;
  padding: 0.3rem 0;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}
.video-link:hover {
  text-decoration: underline;
}

/* CTAãƒœã‚¿ãƒ³ */
.card-cta {
  display: inline-block;
  margin-top: 0.75rem;
  padding: 0.5rem 1.2rem;
  background: linear-gradient(135deg, var(--accent), #E84393);
  color: white;
  border-radius: 2rem;
  text-decoration: none;
  font-size: 0.85rem;
  font-weight: bold;
  transition: all 0.3s ease;
}
.card-cta:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(253,121,168,0.4);
}

/* åºƒå‘Šãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ */
.ad-space {
  background: linear-gradient(135deg, #FFF3E0, #FFE0B2);
  border: 1px dashed #FFB74D;
  border-radius: var(--radius);
  padding: 1.5rem;
  margin: 1.5rem 0;
  text-align: center;
  color: #F57C00;
  font-size: 0.8rem;
}

/* ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ */
.pagination {
  display: flex;
  justify-content: center;
  gap: 0.5rem;
  margin: 2rem 0;
}
.pagination a, .pagination span {
  display: inline-block;
  padding: 0.5rem 1rem;
  border-radius: var(--radius);
  text-decoration: none;
  font-size: 0.9rem;
  border: 1px solid var(--border);
}
.pagination a {
  color: var(--primary);
  background: white;
}
.pagination a:hover {
  background: var(--primary);
  color: white;
}
.pagination .current {
  background: var(--primary);
  color: white;
  border-color: var(--primary);
}

/* ãƒ•ãƒƒã‚¿ãƒ¼ */
.footer {
  background: linear-gradient(135deg, #2D3436, #636E72);
  color: white;
  text-align: center;
  padding: 1.5rem 1rem;
  margin-top: 3rem;
  font-size: 0.8rem;
}
.footer a { color: var(--primary-light); text-decoration: none; }
.footer a:hover { text-decoration: underline; }

/* ç©ºçŠ¶æ…‹ */
.empty-state {
  text-align: center;
  padding: 3rem 1rem;
  color: var(--text-light);
}
.empty-state .emoji { font-size: 3rem; margin-bottom: 1rem; }

/* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ– */
@media (max-width: 600px) {
  .header h1 { font-size: 1.4rem; }
  .card-thumbnail { width: 48px; height: 48px; }
  .card-meta { font-size: 0.75rem; }
}
""".strip()


def render_head(title: str, description: str, url: str = "", og_image: str = "") -> str:
    """HTMLã®headéƒ¨åˆ†ã‚’ç”Ÿæˆ"""
    og_image_tag = f'<meta property="og:image" content="{og_image}">' if og_image else ""
    canonical = f'<link rel="canonical" href="{url}">' if url else ""

    return f"""<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{title}</title>
  <meta name="description" content="{description}">
  <meta property="og:title" content="{title}">
  <meta property="og:description" content="{description}">
  <meta property="og:type" content="website">
  <meta property="og:url" content="{url}">
  {og_image_tag}
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="{title}">
  <meta name="twitter:description" content="{description}">
  {canonical}
  <link rel="stylesheet" href="style.css">
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-SJ6FD6ZGJE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){{dataLayer.push(arguments);}}
    gtag('js', new Date());
    gtag('config', 'G-SJ6FD6ZGJE');
  </script>
</head>"""


def render_header() -> str:
    """ã‚µã‚¤ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    return f"""
<header class="header">
  <h1>{SITE_NAME}</h1>
  <p>{SITE_TAGLINE}</p>
</header>"""


def render_vtuber_card(vtuber: dict, index: int = 0) -> str:
    """VTuberã‚«ãƒ¼ãƒ‰HTMLã‚’ç”Ÿæˆ"""
    name = vtuber.get("title", "åå‰ä¸æ˜")
    thumbnail = vtuber.get("thumbnail", "")
    sub_count = vtuber.get("subscriber_count", 0)
    intro = vtuber.get("introduction", "")
    channel_id = vtuber.get("channel_id", "")
    channel_url = f"https://www.youtube.com/channel/{channel_id}"
    pub_date = vtuber.get("published_at", "")
    videos = vtuber.get("latest_videos", [])

    # ãƒ¡ã‚¿æƒ…å ±
    meta_parts = []
    if sub_count > 0:
        meta_parts.append(f"<span>ğŸ“Š {format_subscriber_count(sub_count)}</span>")
    elif sub_count == -1:
        meta_parts.append("<span>ğŸ“Š éå…¬é–‹</span>")
    if pub_date:
        meta_parts.append(f"<span>ğŸ“… {format_date_jp(pub_date)}\u00A0é–‹è¨­</span>")

    meta_html = "\n          ".join(meta_parts)

    # ç´¹ä»‹æ–‡
    intro_html = ""
    if intro:
        intro_escaped = intro.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace("\n", "<br>")
        intro_html = f'<div class="card-intro">{intro_escaped}</div>'

    # å‹•ç”»ãƒªã‚¹ãƒˆ
    videos_html = ""
    if videos:
        video_links = ""
        for v in videos[:3]:
            vid = v.get("video_id", "")
            vtitle = v.get("title", "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
            video_links += f'<a href="https://www.youtube.com/watch?v={vid}" target="_blank" rel="noopener" class="video-link">â–¶ {vtitle}</a>\n'
        videos_html = f"""
      <div class="card-videos">
        <div class="card-videos-title">æœ€è¿‘ã®å‹•ç”»</div>
        {video_links}
      </div>"""

    delay = index * 0.05

    return f"""
    <article class="vtuber-card" style="animation-delay: {delay}s">
      <div class="card-header">
        <img src="{thumbnail}" alt="{name}" class="card-thumbnail" loading="lazy"
             onerror="this.src='data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%2264%22 height=%2264%22><rect fill=%22%236C5CE7%22 width=%2264%22 height=%2264%22 rx=%2232%22/><text x=%2232%22 y=%2240%22 fill=%22white%22 text-anchor=%22middle%22 font-size=%2224%22>?</text></svg>'">
        <div class="card-info">
          <div class="card-name">
            <a href="{channel_url}" target="_blank" rel="noopener">{name}</a>
          </div>
          <div class="card-meta">
            {meta_html}
          </div>
        </div>
      </div>
      {intro_html}
      {videos_html}
      <a href="{channel_url}" target="_blank" rel="noopener" class="card-cta">ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¦‹ã‚‹ â†’</a>
    </article>"""


def render_ad_space() -> str:
    """åºƒå‘Šãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    return """
    <div class="ad-space">
      ğŸ“¢ åºƒå‘Šã‚¹ãƒšãƒ¼ã‚¹ï¼ˆnendå¯©æŸ»é€šéå¾Œã«è¡¨ç¤ºï¼‰
    </div>"""


def render_pagination(current_page: int, total_pages: int) -> str:
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³HTMLã‚’ç”Ÿæˆ"""
    if total_pages <= 1:
        return ""

    parts = ['<div class="pagination">']
    for i in range(1, total_pages + 1):
        filename = "index.html" if i == 1 else f"page{i}.html"
        if i == current_page:
            parts.append(f'  <span class="current">{i}</span>')
        else:
            parts.append(f'  <a href="{filename}">{i}</a>')
    parts.append("</div>")

    return "\n".join(parts)


def render_footer() -> str:
    """ãƒ•ãƒƒã‚¿ãƒ¼HTMLã‚’ç”Ÿæˆ"""
    now = datetime.now(timezone.utc) + timedelta(hours=9)  # JST
    update_time = now.strftime("%Y/%m/%d %H:%M")

    return f"""
<footer class="footer">
  <p>{SITE_NAME} | æœ€çµ‚æ›´æ–°: {update_time} JST</p>
  <p>ãŠå•ã„åˆã‚ã›ãƒ»æ²è¼‰å‰Šé™¤ä¾é ¼ã¯<a href="mailto:contact@vtuber-matome.net">ã“ã¡ã‚‰</a></p>
  <p style="margin-top: 0.5rem; font-size: 0.7rem; opacity: 0.7;">
    å½“ã‚µã‚¤ãƒˆã¯YouTubeã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«æ–°äººVTuberã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚
  </p>
</footer>"""


def generate_index_page(approved: list, page: int, total_pages: int) -> str:
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®HTMLã‚’ç”Ÿæˆ"""
    start = (page - 1) * ITEMS_PER_PAGE
    end = start + ITEMS_PER_PAGE
    page_items = approved[start:end]

    title = f"{SITE_NAME} - {SITE_TAGLINE}"
    description = "æ–°äººVTuberã‚’æ¯æ—¥ç™ºæ˜ãƒ»ç´¹ä»‹ï¼ã‚ãªãŸã®æ–°ã—ã„æ¨ã—ãŒè¦‹ã¤ã‹ã‚‹ã‹ã‚‚ã€‚"

    cards_html = ""
    for i, vtuber in enumerate(page_items):
        cards_html += render_vtuber_card(vtuber, i)
        # 5ä»¶ã”ã¨ã«åºƒå‘Š
        if (i + 1) % 5 == 0 and i < len(page_items) - 1:
            cards_html += render_ad_space()

    if not page_items:
        cards_html = """
    <div class="empty-state">
      <div class="emoji">ğŸ”</div>
      <p>ã¾ã ç´¹ä»‹æ¸ˆã¿ã®VTuberãŒã„ã¾ã›ã‚“ã€‚</p>
      <p>ã¾ã‚‚ãªãæ–°äººVTuberã®ç´¹ä»‹ãŒå§‹ã¾ã‚Šã¾ã™ï¼</p>
    </div>"""

    page_url = SITE_URL if page == 1 else f"{SITE_URL}/page{page}.html"

    return f"""{render_head(title, description, page_url)}
<body>
  {render_header()}
  <main class="container">
    <div class="section-title">âœ¨ æ–°äººVTuberç´¹ä»‹ï¼ˆ{len(approved)}äººï¼‰</div>
    {cards_html}
    {render_pagination(page, total_pages)}
  </main>
  {render_footer()}
</body>
</html>"""


def generate_vtuber_page(vtuber: dict) -> str:
    """å€‹åˆ¥VTuberç´¹ä»‹ãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    name = vtuber.get("title", "åå‰ä¸æ˜")
    channel_id = vtuber.get("channel_id", "")
    slug = channel_id_hash(channel_id)
    thumbnail = vtuber.get("thumbnail", "")
    intro = vtuber.get("introduction", "")
    channel_url = f"https://www.youtube.com/channel/{channel_id}"
    videos = vtuber.get("latest_videos", [])

    title = f"ã€æ–°äººVTuberã€‘{name}ã•ã‚“ãŒãƒ‡ãƒ“ãƒ¥ãƒ¼ï¼ | {SITE_NAME}"
    description = intro[:120] if intro else f"{name}ã•ã‚“ã®ç´¹ä»‹ãƒšãƒ¼ã‚¸"
    page_url = f"{SITE_URL}/vtuber/{slug}.html"

    # å‹•ç”»åŸ‹ã‚è¾¼ã¿
    videos_html = ""
    if videos:
        videos_section = ""
        for v in videos[:3]:
            vid = v.get("video_id", "")
            vtitle = v.get("title", "")
            videos_section += f"""
      <div style="margin-bottom: 1rem;">
        <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:8px;">
          <iframe src="https://www.youtube.com/embed/{vid}" frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen
                  style="position:absolute;top:0;left:0;width:100%;height:100%;"
                  loading="lazy"></iframe>
        </div>
        <p style="font-size:0.85rem;color:var(--text-light);margin-top:0.5rem;">{vtitle}</p>
      </div>"""
        videos_html = f"""
    <div class="section-title">ğŸ¬ æœ€è¿‘ã®å‹•ç”»</div>
    {videos_section}"""

    intro_escaped = ""
    if intro:
        intro_escaped = intro.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace("\n", "<br>")

    return f"""{render_head(title, description, page_url, thumbnail)}
<body>
  {render_header()}
  <main class="container">
    <a href="/" style="display:inline-block;margin-bottom:1rem;color:var(--primary);text-decoration:none;">â† ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹</a>

    <article class="vtuber-card" style="animation-delay:0s">
      <div class="card-header">
        <img src="{thumbnail}" alt="{name}" class="card-thumbnail" loading="lazy">
        <div class="card-info">
          <div class="card-name" style="font-size:1.3rem;">{name}</div>
          <div class="card-meta">
            <span>ğŸ“Š {format_subscriber_count(vtuber.get('subscriber_count', 0))}</span>
            <span>ğŸ“… {format_date_jp(vtuber.get('published_at', ''))}\u00A0é–‹è¨­</span>
          </div>
        </div>
      </div>
      <div class="card-intro" style="font-size:1rem;">
        {intro_escaped}
      </div>
      <a href="{channel_url}" target="_blank" rel="noopener" class="card-cta" style="margin-top:1rem;">
        ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¦‹ã‚‹ â†’
      </a>
    </article>

    {videos_html}

    {render_ad_space()}
  </main>
  {render_footer()}
</body>
</html>"""


# ============================================================
# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
# ============================================================

def generate_rss(approved: list) -> str:
    """RSS 2.0ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    now = datetime.now(timezone.utc)
    pub_date = now.strftime("%a, %d %b %Y %H:%M:%S +0000")

    items = ""
    sorted_list = sorted(
        approved,
        key=lambda x: x.get("approved_at", x.get("discovered_at", "")),
        reverse=True,
    )[:20]  # æœ€æ–°20ä»¶

    for vtuber in sorted_list:
        name = vtuber.get("title", "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
        slug = channel_id_hash(vtuber.get("channel_id", ""))
        link = f"{SITE_URL}/vtuber/{slug}.html"
        intro = vtuber.get("introduction", "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
        channel_url = f"https://www.youtube.com/channel/{vtuber.get('channel_id', '')}"
        sub_count = format_subscriber_count(vtuber.get("subscriber_count", 0))
        approved_at = vtuber.get("approved_at", now.isoformat())
        dt = parse_iso8601(approved_at)
        item_date = dt.strftime("%a, %d %b %Y %H:%M:%S +0000")

        description = f"{intro} ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…: {sub_count} #æ–°äººVTuber"

        items += f"""    <item>
      <title>ã€æ–°äººVTuberã€‘{name}ã•ã‚“ãŒãƒ‡ãƒ“ãƒ¥ãƒ¼ï¼</title>
      <link>{link}</link>
      <description>{description}</description>
      <pubDate>{item_date}</pubDate>
      <guid isPermaLink="true">{link}</guid>
    </item>
"""

    return f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>{SITE_NAME}</title>
    <link>{SITE_URL}</link>
    <description>{SITE_TAGLINE}</description>
    <language>ja</language>
    <lastBuildDate>{pub_date}</lastBuildDate>
    <atom:link href="{SITE_URL}/feed.xml" rel="self" type="application/rss+xml"/>
{items}  </channel>
</rss>"""


def generate_robots_txt() -> str:
    return f"""User-agent: *
Allow: /
Sitemap: {SITE_URL}/sitemap.xml"""


def generate_sitemap(approved: list) -> str:
    now = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    urls = [f"""  <url>
    <loc>{SITE_URL}/</loc>
    <lastmod>{now}</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
  </url>"""]

    for vtuber in approved:
        slug = channel_id_hash(vtuber.get("channel_id", ""))
        urls.append(f"""  <url>
    <loc>{SITE_URL}/vtuber/{slug}.html</loc>
    <lastmod>{now}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>""")

    return f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{chr(10).join(urls)}
</urlset>"""


def write_all_files(approved: list):
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãå‡ºã™"""
    print("\nHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")

    # CSS
    css_path = PUBLIC_DIR / "style.css"
    css_path.write_text(render_css(), encoding="utf-8")
    print(f"  âœ… style.css")

    # æ‰¿èªæ¸ˆã¿ã‚’æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_approved = sorted(
        approved,
        key=lambda x: x.get("approved_at", x.get("discovered_at", "")),
        reverse=True,
    )

    # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    total_pages = max(1, math.ceil(len(sorted_approved) / ITEMS_PER_PAGE))
    for page in range(1, total_pages + 1):
        filename = "index.html" if page == 1 else f"page{page}.html"
        html = generate_index_page(sorted_approved, page, total_pages)
        (PUBLIC_DIR / filename).write_text(html, encoding="utf-8")
        print(f"  âœ… {filename}")

    # å€‹åˆ¥VTuberãƒšãƒ¼ã‚¸
    vtuber_dir = PUBLIC_DIR / "vtuber"
    vtuber_dir.mkdir(exist_ok=True)
    for vtuber in sorted_approved:
        slug = channel_id_hash(vtuber.get("channel_id", ""))
        html = generate_vtuber_page(vtuber)
        (vtuber_dir / f"{slug}.html").write_text(html, encoding="utf-8")
    print(f"  âœ… å€‹åˆ¥ãƒšãƒ¼ã‚¸: {len(sorted_approved)}ä»¶")

    # robots.txt & sitemap.xml & CNAME & RSS
    (PUBLIC_DIR / "robots.txt").write_text(generate_robots_txt(), encoding="utf-8")
    (PUBLIC_DIR / "sitemap.xml").write_text(generate_sitemap(sorted_approved), encoding="utf-8")
    (PUBLIC_DIR / "feed.xml").write_text(generate_rss(sorted_approved), encoding="utf-8")
    (PUBLIC_DIR / "CNAME").write_text("vtuber-matome.net", encoding="utf-8")
    print(f"  âœ… robots.txt & sitemap.xml & feed.xml & CNAME")

    print(f"\nç”Ÿæˆå®Œäº†ï¼ åˆè¨ˆ {total_pages + len(sorted_approved) + 3} ãƒ•ã‚¡ã‚¤ãƒ«")


# ============================================================
# CLI
# ============================================================

def print_candidates(candidates: list):
    """å€™è£œãƒªã‚¹ãƒˆã‚’è¡¨ç¤º"""
    pending = [c for c in candidates if c.get("status") == "pending"]
    if not pending:
        print("æ‰¿èªå¾…ã¡ã®å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"\nğŸ“‹ æ‰¿èªå¾…ã¡ã®å€™è£œ: {len(pending)}ä»¶")
    print("-" * 60)
    for i, c in enumerate(pending, 1):
        sub = format_subscriber_count(c.get("subscriber_count", 0))
        age = days_ago(c.get("published_at", datetime.now(timezone.utc).isoformat()))
        print(f"  {i}. {c['title']}")
        print(f"     ç™»éŒ²è€…: {sub} | é–‹è¨­: {age}æ—¥å‰ | å‹•ç”»: {c.get('video_count', 0)}æœ¬")
        print(f"     ID: {c['channel_id']}")
        print()


def cli_approve(candidates: list) -> list:
    """CLIã‹ã‚‰å€™è£œã‚’æ‰¿èªã™ã‚‹"""
    pending = [c for c in candidates if c.get("status") == "pending"]
    if not pending:
        print("æ‰¿èªå¾…ã¡ã®å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return candidates

    print_candidates(candidates)
    print("æ‰¿èªã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ç•ªå·ã‚’å…¥åŠ›ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯ã€qã§çµ‚äº†ï¼‰:")

    while True:
        user_input = input("> ").strip()
        if user_input.lower() == "q":
            break

        try:
            indices = [int(x.strip()) for x in user_input.split(",")]
            for idx in indices:
                if 1 <= idx <= len(pending):
                    channel_id = pending[idx - 1]["channel_id"]
                    candidates, approved = approve_candidate(candidates, channel_id)
                    if approved:
                        print(f"  âœ… æ‰¿èª: {approved['title']}")
                else:
                    print(f"  âŒ ç„¡åŠ¹ãªç•ªå·: {idx}")
        except ValueError:
            print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        # ãƒªã‚¹ãƒˆæ›´æ–°
        pending = [c for c in candidates if c.get("status") == "pending"]
        if not pending:
            print("å…¨å€™è£œã‚’å‡¦ç†ã—ã¾ã—ãŸã€‚")
            break

    return candidates


# ============================================================
# ãƒ¡ã‚¤ãƒ³
# ============================================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    ensure_dirs()

    if not YOUTUBE_API_KEY:
        print("[ERROR] YOUTUBE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ç’°å¢ƒå¤‰æ•° YOUTUBE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§å‹•ä½œã‚’å¤‰ãˆã‚‹
    mode = sys.argv[1] if len(sys.argv) > 1 else "collect"

    if mode == "collect":
        # å€™è£œåé›†ï¼ˆè‡ªå‹•å®Ÿè¡Œç”¨ï¼‰
        candidates = collect_candidates()
        save_json(CANDIDATES_FILE, candidates)
        print_candidates(candidates)

        # æ‰¿èªæ¸ˆã¿ã®HTMLã‚‚å†ç”Ÿæˆ
        approved = load_json(APPROVED_FILE, [])
        write_all_files(approved)

    elif mode == "approve":
        # æ‰¿èªå‡¦ç†ï¼ˆæ‰‹å‹•å®Ÿè¡Œç”¨ï¼‰
        candidates = load_json(CANDIDATES_FILE, [])
        candidates = cli_approve(candidates)
        save_json(CANDIDATES_FILE, candidates)

    elif mode == "approve-all":
        # å…¨å€™è£œã‚’ä¸€æ‹¬æ‰¿èªï¼ˆGitHub Actionsç”¨ï¼‰
        candidates = load_json(CANDIDATES_FILE, [])
        pending = [c for c in candidates if c.get("status") == "pending"]
        print(f"\nå…¨{len(pending)}ä»¶ã‚’ä¸€æ‹¬æ‰¿èªã—ã¾ã™...")
        for c in pending:
            candidates, approved_entry = approve_candidate(candidates, c["channel_id"])
            if approved_entry:
                print(f"  âœ… {approved_entry['title']}")
        save_json(CANDIDATES_FILE, candidates)

        # HTMLå†ç”Ÿæˆ
        approved = load_json(APPROVED_FILE, [])
        write_all_files(approved)

    elif mode == "generate":
        # HTMLç”Ÿæˆã®ã¿
        approved = load_json(APPROVED_FILE, [])
        write_all_files(approved)

    elif mode == "status":
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        candidates = load_json(CANDIDATES_FILE, [])
        approved = load_json(APPROVED_FILE, [])
        pending = [c for c in candidates if c.get("status") == "pending"]
        print(f"\nğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
        print(f"  å€™è£œï¼ˆæœªå‡¦ç†ï¼‰: {len(pending)}ä»¶")
        print(f"  æ‰¿èªæ¸ˆã¿: {len(approved)}ä»¶")
        print(f"  åˆè¨ˆå€™è£œ: {len(candidates)}ä»¶")

    else:
        print(f"Unknown mode: {mode}")
        print("Usage: python scrape.py [collect|approve|approve-all|generate|status]")
        sys.exit(1)


if __name__ == "__main__":
    main()
